{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Med-I-C · AMR-Guard\n",
    "### Infection Lifecycle Orchestrator — Kaggle Demo\n",
    "\n",
    "**Steps**\n",
    "1. Clone repo & install packages\n",
    "2. Authenticate with Hugging Face\n",
    "3. Download models\n",
    "4. Initialise the knowledge base\n",
    "5. Launch the Streamlit app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 · Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, torch\n",
    "\n",
    "# GPU check\n",
    "print(subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
    "                     capture_output=True, text=True).stdout.strip())\n",
    "print(f\"PyTorch {torch.__version__} · CUDA {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone the repo (skip if already present)\n",
    "if [ ! -d /kaggle/working/Med-I-C ]; then\n",
    "    git clone https://github.com/benghita/Med-I-C.git /kaggle/working/Med-I-C\n",
    "else\n",
    "    echo \"Repo already cloned — pulling latest changes\"\n",
    "    git -C /kaggle/working/Med-I-C pull\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install packages from pyproject.toml dependencies\n",
    "!pip install -q \\\n",
    "    \"langgraph>=0.0.15\" \"langchain>=0.3.0\" langchain-text-splitters langchain-community \\\n",
    "    \"chromadb>=0.4.0\" sentence-transformers \\\n",
    "    \"transformers>=4.50.0\" accelerate bitsandbytes \\\n",
    "    streamlit huggingface_hub \\\n",
    "    \"pydantic>=2.0\" python-dotenv openpyxl pypdf \"pandas>=2.0\" jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 · Hugging Face Authentication\n",
    "\n",
    "Add your token to **Kaggle → Add-ons → Secrets** as `HF_TOKEN`.\n",
    "\n",
    "Accept model licences before running:\n",
    "- https://huggingface.co/google/gemma-2-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    hf_token = UserSecretsClient().get_secret(\"HF_TOKEN\")\n",
    "    print(\"Token loaded from Kaggle secrets\")\n",
    "except Exception:\n",
    "    hf_token = os.getenv(\"HF_TOKEN\", \"\")\n",
    "    print(\"Token loaded from environment\" if hf_token else \"WARNING: No HF_TOKEN found\")\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 · Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Single model used for all agents in the demo\n",
    "MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "\n",
    "print(f\"Downloading {MODEL_ID} …\")\n",
    "snapshot_download(\n",
    "    repo_id=MODEL_ID,\n",
    "    ignore_patterns=[\"*.gguf\", \"*.ot\"],   # skip quantised formats we don't need\n",
    ")\n",
    "print(\"Download complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model for RAG (small, fast)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Embedding model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 · Configure & Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write .env for the Kaggle environment\n",
    "env_content = f\"\"\"\n",
    "MEDIC_ENV=kaggle\n",
    "MEDIC_DEFAULT_BACKEND=local\n",
    "MEDIC_USE_VERTEX=false\n",
    "MEDIC_QUANTIZATION=4bit\n",
    "\n",
    "MEDIC_LOCAL_MEDGEMMA_4B_MODEL={MODEL_ID}\n",
    "MEDIC_LOCAL_MEDGEMMA_27B_MODEL={MODEL_ID}\n",
    "MEDIC_LOCAL_TXGEMMA_9B_MODEL={MODEL_ID}\n",
    "MEDIC_LOCAL_TXGEMMA_2B_MODEL={MODEL_ID}\n",
    "\n",
    "MEDIC_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\n",
    "MEDIC_DATA_DIR=/kaggle/working/Med-I-C/data\n",
    "MEDIC_CHROMA_DB_DIR=/kaggle/working/Med-I-C/data/chroma_db\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(\"/kaggle/working/Med-I-C/.env\", \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\".env written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/kaggle/working/Med-I-C\")\n",
    "\n",
    "# Initialise SQLite + ChromaDB knowledge base\n",
    "!python /kaggle/working/Med-I-C/setup_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 · Launch the App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, threading, time, requests\n",
    "\n",
    "# Start Streamlit in the background\n",
    "streamlit_proc = subprocess.Popen(\n",
    "    [\"streamlit\", \"run\", \"/kaggle/working/Med-I-C/app.py\",\n",
    "     \"--server.port\", \"8501\",\n",
    "     \"--server.headless\", \"true\",\n",
    "     \"--server.enableCORS\", \"false\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "\n",
    "# Wait for Streamlit to be ready\n",
    "for _ in range(15):\n",
    "    try:\n",
    "        if requests.get(\"http://localhost:8501\", timeout=2).status_code == 200:\n",
    "            print(\"Streamlit is running on port 8501\")\n",
    "            break\n",
    "    except Exception:\n",
    "        time.sleep(2)\n",
    "else:\n",
    "    print(\"Streamlit may still be starting…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expose via localtunnel — the public URL will appear below\n",
    "tunnel_proc = subprocess.Popen(\n",
    "    [\"npx\", \"localtunnel\", \"--port\", \"8501\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.DEVNULL,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "# Print the public URL\n",
    "for line in tunnel_proc.stdout:\n",
    "    if \"https://\" in line:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"  App URL: {line.strip()}\")\n",
    "        print(\"=\"*50)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
