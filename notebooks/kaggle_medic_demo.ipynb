{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0846ff4",
   "metadata": {},
   "source": [
    "# Med-I-C · AMR-Guard\n",
    "### Infection Lifecycle Orchestrator — Kaggle Demo\n",
    "\n",
    "| Agent | Role | Model |\n",
    "|---|---|---|\n",
    "| 1 · Intake Historian | Patient data, CrCl, MDR risk | MedGemma 4B IT |\n",
    "| 2 · Vision Specialist | Lab report → structured JSON | MedGemma 4B IT |\n",
    "| 3 · Trend Analyst | MIC creep, resistance velocity | MedGemma 27B Text IT ¹ |\n",
    "| 4 · Clinical Pharmacologist | Final Rx + safety check | MedGemma 4B IT + TxGemma 9B ¹ |\n",
    "\n",
    "> ¹ Substituted with smaller variants on Kaggle T4 (16 GB GPU) — see Section 3.\n",
    "\n",
    "**Steps:** Clone → Install → Authenticate → Download models → Init KB → Launch app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a735d8ce",
   "metadata": {},
   "source": [
    "## 1 · Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, torch\n",
    "\n",
    "gpu_info = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
    "    capture_output=True, text=True\n",
    ").stdout.strip()\n",
    "print(f\"GPU  : {gpu_info}\")\n",
    "print(f\"Torch: {torch.__version__} · CUDA {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c637bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d /kaggle/working/Med-I-C ]; then\n",
    "    git clone https://github.com/benghita/Med-I-C.git /kaggle/working/Med-I-C\n",
    "else\n",
    "    echo \"Repo already present — pulling latest\"\n",
    "    git -C /kaggle/working/Med-I-C pull\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q \\\n",
    "    \"langgraph>=0.0.15\" \"langchain>=0.3.0\" langchain-text-splitters langchain-community \\\n",
    "    \"chromadb>=0.4.0\" sentence-transformers \\\n",
    "    \"transformers>=4.50.0\" accelerate bitsandbytes \\\n",
    "    streamlit huggingface_hub \\\n",
    "    \"pydantic>=2.0\" python-dotenv openpyxl pypdf \"pandas>=2.0\" jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d4d05",
   "metadata": {},
   "source": [
    "## 2 · Hugging Face Authentication\n",
    "\n",
    "Add your token to **Kaggle → Add-ons → Secrets** as `HF_TOKEN`.\n",
    "\n",
    "Accept the model licences **before** running this notebook:\n",
    "- MedGemma 4B IT → https://huggingface.co/google/medgemma-4b-it\n",
    "- TxGemma 2B → https://huggingface.co/google/txgemma-2b-predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    hf_token = UserSecretsClient().get_secret(\"HF_TOKEN\")\n",
    "    print(\"Token loaded from Kaggle secrets\")\n",
    "except Exception:\n",
    "    hf_token = os.getenv(\"HF_TOKEN\", \"\")\n",
    "    print(\"Token loaded from environment\" if hf_token else \"WARNING: No HF_TOKEN found\")\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fee3dc",
   "metadata": {},
   "source": [
    "## 3 · Download Models\n",
    "\n",
    "| Model | Agent | VRAM (4-bit) | Kaggle T4 |\n",
    "|---|---|---|---|\n",
    "| `google/medgemma-4b-it` | 1, 2, 4 primary | ~3 GB | ✓ |\n",
    "| `google/medgemma-27b-text-it` | 3 (Trend Analyst) | ~14 GB | marginal — using 4B sub |\n",
    "| `google/txgemma-9b-predict` | 4 safety check | ~5 GB | ✓ (stacked with 4B: ~8 GB) |\n",
    "| `google/txgemma-2b-predict` | 4 safety fallback | ~1.5 GB | ✓ |\n",
    "\n",
    "**Kaggle strategy:** download `medgemma-4b-it` and `txgemma-2b-predict`.\n",
    "Agent 3 is pointed at `medgemma-4b-it` (4B sub), safety check at `txgemma-2b-predict` (2B sub).\n",
    "Swap to the full 27B / 9B on a machine with ≥ 24 GB VRAM by editing the env cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0586ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "MEDGEMMA_4B   = \"google/medgemma-4b-it\"       # Agents 1, 2, 4 + Agent 3 sub\n",
    "TXGEMMA_2B    = \"google/txgemma-2b-predict\"   # Agent 4 safety sub\n",
    "\n",
    "# Full models for high-VRAM machines (uncomment to use instead)\n",
    "# MEDGEMMA_27B = \"google/medgemma-27b-text-it\"  # Agent 3 — needs ~14 GB in 4bit\n",
    "# TXGEMMA_9B   = \"google/txgemma-9b-predict\"    # Agent 4 safety — needs ~5 GB in 4bit\n",
    "\n",
    "for repo_id in [MEDGEMMA_4B, TXGEMMA_2B]:\n",
    "    print(f\"Downloading {repo_id} …\")\n",
    "    snapshot_download(repo_id=repo_id, ignore_patterns=[\"*.gguf\", \"*.ot\"])\n",
    "    print(f\"  done\")\n",
    "\n",
    "print(\"\\nAll models downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179589bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model for RAG (CPU-only, no licence needed)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Embedding model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea78ed72",
   "metadata": {},
   "source": [
    "## 4 · Configure & Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write .env — edit MEDGEMMA_27B / TXGEMMA_9B lines to use full models on high-VRAM machines\n",
    "env = f\"\"\"\n",
    "MEDIC_ENV=kaggle\n",
    "MEDIC_DEFAULT_BACKEND=local\n",
    "MEDIC_USE_VERTEX=false\n",
    "MEDIC_QUANTIZATION=4bit\n",
    "\n",
    "# Agent 1, 2, 4 — MedGemma 4B IT\n",
    "MEDIC_LOCAL_MEDGEMMA_4B_MODEL={MEDGEMMA_4B}\n",
    "\n",
    "# Agent 3 — MedGemma 27B Text IT  (subbed with 4B for Kaggle T4)\n",
    "# To use full 27B: set to google/medgemma-27b-text-it\n",
    "MEDIC_LOCAL_MEDGEMMA_27B_MODEL={MEDGEMMA_4B}\n",
    "\n",
    "# Agent 4 safety — TxGemma 9B  (subbed with 2B for Kaggle T4)\n",
    "# To use full 9B: set to google/txgemma-9b-predict\n",
    "MEDIC_LOCAL_TXGEMMA_9B_MODEL={TXGEMMA_2B}\n",
    "MEDIC_LOCAL_TXGEMMA_2B_MODEL={TXGEMMA_2B}\n",
    "\n",
    "MEDIC_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\n",
    "MEDIC_DATA_DIR=/kaggle/working/Med-I-C/data\n",
    "MEDIC_CHROMA_DB_DIR=/kaggle/working/Med-I-C/data/chroma_db\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(\"/kaggle/working/Med-I-C/.env\", \"w\") as f:\n",
    "    f.write(env)\n",
    "\n",
    "print(\".env written\")\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/kaggle/working/Med-I-C\")\n",
    "\n",
    "# Populate SQLite + ChromaDB knowledge base from data files\n",
    "!python /kaggle/working/Med-I-C/setup_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d17f6b",
   "metadata": {},
   "source": [
    "## 5 · Launch the App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, requests\n",
    "\n",
    "streamlit_proc = subprocess.Popen(\n",
    "    [\"streamlit\", \"run\", \"/kaggle/working/Med-I-C/app.py\",\n",
    "     \"--server.port\", \"8501\",\n",
    "     \"--server.headless\", \"true\",\n",
    "     \"--server.enableCORS\", \"false\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "\n",
    "for _ in range(15):\n",
    "    try:\n",
    "        if requests.get(\"http://localhost:8501\", timeout=2).status_code == 200:\n",
    "            print(\"Streamlit running on :8501\")\n",
    "            break\n",
    "    except Exception:\n",
    "        time.sleep(2)\n",
    "else:\n",
    "    print(\"Streamlit may still be starting…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ecfb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnel_proc = subprocess.Popen(\n",
    "    [\"npx\", \"localtunnel\", \"--port\", \"8501\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.DEVNULL,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "for line in tunnel_proc.stdout:\n",
    "    if \"https://\" in line:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"  App URL: {line.strip()}\")\n",
    "        print(\"=\"*50)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
